version: 2.1
orbs:
  aws-cli: circleci/aws-cli@2.0.3
  kubernetes: circleci/kubernetes@0.12.0
  aws-eks: circleci/aws-eks@1.1.0
  newman: postman/newman@0.0.2
jobs:
  build:
    docker:
      - image: cimg/python:3.8.5
    steps:
      - checkout
      - restore_cache:
          keys:
            - v1-dependencies-{{ checksum "requirements/common.txt" }}-{{ checksum "requirements/Development.txt" }}
            - v1-dependencies-
      - run:
          name: Install Flask dependencies w/pylint
          command: |
            make setup
            . ~/.cex-api/bin/activate
            make install-dev
      - save_cache:
          paths: ["~/.cex-api"]
          key: v1-dependencies-{{ checksum "requirements/common.txt" }}-{{ checksum "requirements/Development.txt" }}
      - run:
          name: Install Hadolint
          command: |
            sudo wget -O /bin/hadolint https://github.com/hadolint/hadolint/releases/download/v2.5.0/hadolint-Linux-x86_64
            sudo chmod +x /bin/hadolint
      - run:
          name: Run linters
          command: |
            . ~/.cex-api/bin/activate
            make lint
  deploy-docker-image:
    docker:
      - image: cimg/base:2021.07
    steps:
      - checkout
      - setup_remote_docker:
          version: 20.10.6
      - run:
          name: Build and run docker image in detached mode
          command: |
            chmod +x run_docker.sh
            ./run_docker.sh
      - run:
          name: Test that Docker container is up and running
          command: |
            if docker exec $(cat container_id.log) curl --retry 10 --retry-connrefused http://localhost:5000 | grep -s "PRODUCTION"
            then
              echo "Dockerized Flask app is up and running!"
            else
              echo "ERROR: Docker app is down! Exiting..."
              exit 1
            fi
      - run:
          name: Authenticate and upload Docker image to DockerHub
          command: |
            echo "$DOCKERHUB_PASSWORD" | docker login -u "$DOCKERHUB_USERNAME" --password-stdin
            docker push ${DOCKERHUB_USERNAME}/currency-exchange:prod
  deploy-infrastructure:
    docker:
      - image: cimg/base:2021.07
    steps: 
      - checkout
      - aws-cli/install
      - aws-eks/install-eksctl
      - kubernetes/install-kubectl
      - run:
          name: Deploy EKS Cluster, Network & Cluster Cloudformation 
          no_output_timeout: 25m
          command: |
            cd .circleci/cloudformation

            ./deploy-stack.sh -n udacity-capstone-network \
              -f stacks/network.yml \
              -t "project=udacity-capstone workflow=${CIRCLE_WORKFLOW_ID}"

            ./deploy-stack.sh -n udacity-capstone-cluster \
              -f stacks/cluster.yml -c \
              -t "project=udacity-capstone workflow=${CIRCLE_WORKFLOW_ID}"
      - run:
          name: Update kubeconfig and check that svc is running
          command: |
            aws eks update-kubeconfig \
              --region eu-west-2 \
              --name udacity-cluster
  
            kubectl get svc
      - run:
          name: Associate OIDC provider with EKS cluster, if not already associated
          command: |
            eksctl utils associate-iam-oidc-provider --cluster udacity-cluster --approve
      - run:
          name: Create service account and attach CNI addon
          command: |
            eksctl create iamserviceaccount \
              --name aws-node \
              --namespace kube-system \
              --cluster udacity-cluster \
              --attach-policy-arn arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy \
              --approve \
              --override-existing-serviceaccounts
      - run:
          name: Deploy EKS Nodegroup (Private Subnets)
          no_output_timeout: 15m
          command: |
            cd .circleci/cloudformation

            ./deploy-stack.sh -n udacity-capstone-nodegroup \
              -f stacks/nodegroup.yml -c \
              -t "project=udacity-capstone workflow=${CIRCLE_WORKFLOW_ID}"
      - persist_to_workspace:
          root:  ~/.kube
          paths:
            - config
  configure-infrastructure:
    docker:
      - image: cimg/base:2021.07
    steps: 
      - checkout
      - aws-cli/install
      - aws-eks/install-eksctl
      - kubernetes/install-kubectl
      - attach_workspace:
          at: ~/.kube
      - run:
          name: Deploy AWS Load Balancer Controller, if not already deployed
          command: |
            if ! ( kubectl get deployment -n kube-system aws-load-balancer-controller ) ; then
              eksctl create iamserviceaccount \
                --cluster=udacity-cluster \
                --namespace=kube-system \
                --name=aws-load-balancer-controller \
                --attach-policy-arn=arn:aws:iam::${AWS_Account_Id}:policy/AWSLoadBalancerControllerIAMPolicy \
                --override-existing-serviceaccounts \
                --approve
              
              kubectl apply \
                  --validate=false \
                  -f https://github.com/jetstack/cert-manager/releases/download/v1.1.1/cert-manager.yaml
              
              cd kubernetes/
              kubectl apply -f aws-load-balancer-controller-v2.2.0.yaml
              echo "Load balancer Controller has been deployed!"
              exit 0
            fi
            echo "Load balancer Controller is already deployed"
      - run:
          name: Check that AWS controller is running
          command: |
            kubectl get deployment -n kube-system aws-load-balancer-controller
  deploy-app:
    docker:
      - image: cimg/base:2021.07
    steps: 
      - checkout
      - aws-cli/install
      - kubernetes/install-kubectl
      - attach_workspace:
          at: ~/.kube
      - run:
          name: Apply kubernetes resources in rolling update fashion
          command: |        
            cd kubernetes/prod
            kubectl apply -f flask-deployment.yaml
            kubectl apply -f flask-service.yaml
            kubectl apply -f ingress.yml
      - run:
          name: Append Load Balancer IP to a reference file and postman env file
          command: |
            LB_URL=$(kubectl get ingress flask-ingress -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}')
            
            echo $LB_URL > lb_url.txt
            sed -i "s|HOST|${LB_URL}|g" postman/AWS-env.json
      - persist_to_workspace:
          root: ./
          paths:
            - lb_url.txt
            - postman/AWS-env.json
  backend-smoke-test:
    docker:
      - image: cimg/base:2021.07
    steps:
      - checkout
      - attach_workspace:
          at: ./
      - run:
          name: Sleep for 2mins to allow App Load Balancer to be provisioned
          command: |
            sleep 120
      - run:
          name: Backend smoke test via Load Balancer
          command: |
            LB_URL=$(cat lb_url.txt)
            if curl -s --retry 5 --retry-connrefused ${LB_URL} | grep "PRODUCTION"
            then
              echo "Currency Exchange API is live!"
            else
              echo "ERROR: API is down. Exiting..."
              exit 1
            fi
  newman-collection-run:
    executor: newman/postman-newman-docker
    steps:
      - checkout
      - attach_workspace:
          at: ./
      - newman/newman-run:
          collection: postman/CEX-API-Collection-Tests-Prod.postman_collection.json
          environment: postman/AWS-env.json
          iteration-data: postman/test_data.csv
workflows:
  default:
    jobs:
      - build
      - deploy-docker-image:
          requires: [build]
      - deploy-infrastructure:
          requires: [deploy-docker-image]
      - configure-infrastructure:
          requires: [deploy-infrastructure]
      - deploy-app:
          requires: [configure-infrastructure]
      - backend-smoke-test:
          requires: [deploy-app]
